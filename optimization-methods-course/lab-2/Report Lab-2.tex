\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{arcs}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\DeclareGraphicsExtensions{.png, .jpeg}
\graphicspath{{images/}}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{stackrel}
\usepackage{xcolor}
\usepackage{minted}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
}

\begin{document}
\begin{titlepage}
    \bfseries 
        {\centering
            \vspace*{14em}
            \Huge Методы оптимизации\par
            \bigbreak
            Отчёт по лабораторной работе №2 \par
        }
    \vspace{20em}
    \begin{spacing}{2}
        \begin{flushright}
            {\Large \textbf{Выполнили:}}  \\
            {\large Евсеева Арина M3234} \\
            {\large Шульпин Егор M3232} \\
            {\large Гаврилюк Виталий M3232} \\ 
        \end{flushright}
    \end{spacing}
\end{titlepage}

\newpage
\section*{Постановка задачи}
\subsection*{1. Метод Ньютона с постоянным шагом;}
\subsection*{2. Метод Ньютона с использованием одномерного поиска (dichotomy);}
\subsection*{3. Модификации метода Ньютона:}
\begin{itemize}
    \item \textbf{Метод Ньютона с использованием метода сопряженных градиентов (Newton-CG);}
    \item \textbf{Квазиньютоновский метод (BFGS);}
\end{itemize}
\subsection*{4. Сравнение методов на примере функции Розенброка и не полиномиальной функции:}
\begin{itemize}
    \item \textbf{Сравнение своей реализации метода Ньютона с методом Newton-CG из библиотеки scipy.optimize;}
    \item \textbf{Сравнение эффективности методов нулевого порядка и градиентного спуска из лабораторной работы №1 с методом Ньютона и квазиньютоновскими методами;}
    \item \textbf{Сравнение эффективности методов нулевого порядка с квазиньютоновскими методами с вычислением производных разностным методом;}
    \item \textbf{Иллюстрация работы методов с помощью графиков и таблиц;}
\end{itemize}
\subsection*{5. Дополнительная реализация метода Ньютона с использованием одномерного поиска по правилу Вольфе и дальнейшее сравнение;}
\subsection*{6. Исследование реализованных методов в сложных случаях:}
\begin{itemize}
    \item \textbf{Исследование работоспособности метода Ньютона в зависимости от выбранной начальной точки;}
    \item \textbf{Исследование результатов каждого из методов на функции со многими точками локального минимума.}
\end{itemize}
\subsection*{\href{https://gitfront.io/r/Vitaliy-X-0/TLNs9s2iAaGd/MetOpt/}{Итоговая реализация}}

\newpage
\section*{Метод Ньютона с постоянным шагом}
\subsection*{Описание метода}
Метод Ньютона - это итерационный метод нахождения нуля заданной функции, который впервые был предложен Исааком Ньютоном. Данный метод может быть использован для нахождения минимума функции в задачах оптимизации. Известно, что необходимым условием минимума функции в точке $x$ является равенство ее градиента нулю. Таким образом, для поиска минимума мы может воспользоваться методом Ньютона для решения уравнения вида: $\nabla f(x) = 0$. В стандартной реализации метода Ньютона подразумевается, что исходная функция является дифференцируемой по каждому аргументу (возможно, несколько раз).
\subsection*{Релизация метода}
Алгоритм метода Ньютона:
\begin{enumerate}
    \item Выбор начальной точки $x = (x_0, \dots, x_n)$ - начальное приближение.
    \item На каждой итерации вычисляется новое значения аргумента функции:
    
    $x_{i+1} = x_i - H^{-1}(x_i) \cdot \nabla f(x_i)$, где $H$ - матрица Гессе заданной функции.
    \item Условие останова: $||x_{i+1} - x_{i}|| < \mathcal{E}$ \; или \; $||H^{-1} \cdot \nabla f(x_{i})|| < \mathcal{E}$.
\end{enumerate}
\section*{Метод Ньютона с использованием одномерного поиска (dichotomy)}
\subsection*{Реализация метода Ньютона с использованием дихотомии}
Как и в случае с градиентным спуском, использование фиксированного шага может сказаться на работоспособности: замедление приближения к минимуму или его перепрыгивание. Также метод Ньютона очень восприимчив к начальной точке, поэтому при неверно подобранном шаге на каждой итерации может происходить отдоление в неверном направлении. Для поиска оптимального шага можно использовать метод одномерного поиска, например, дихотомии: по каждому направлению подбираем значение на отрезке с границами $l = \mathcal{E}$, $r = 50$.
\subsection*{Реализация метода}
Алгоритм очень схож с предыдущим, однако на каждой итерации будем находить (с помощью дихотомии) $\alpha$ - оптимальный шаг. Тогда $x_{i+1} = x_i - \alpha \cdot H^{-1}(x_i) \cdot \nabla f(x_i)$.
\section*{Модификации метода Ньютона}
\subsection*{Метод Ньютона с использованием метода сопряженных градиентов (Newton-CG)}
\subsubsection*{Описание метода}
Главным недостатком стандартной реализации метода Ньютона является необходимость хранения гессиана, который слишком неэффективен по памяти и вычислению при большом количесвтве данных. Использование метода сопряженных градиентов является модификацией метода Ньютона, позволяющей на каждом шаге выбирать направление с использованием градиентов функции. Данный метод позволяет (приблизительно) обращать гессиан, чем уменьшает сложность вычислений и устраняет проблему с нехваткой памяти.
\subsubsection*{Реализация метода}
Реализация метода Ньютона с использованием метода сопряженных градиентов взята из библиотеки scipy.optimize, анализ эффективности которой будет рассмотрен далее.
\subsection*{Квазиньютоновский метод (BFGS)}
\subsubsection*{Описание метода}
Метод BFGS (Broyden–Fletcher–Goldfarb–Shanno) - это метод для решения задач оптимизации без ограничений. Данный метод относится к классу квазиньютоноских, которые вычисляют приближенное значение обратного гессиана с помощью информации о градиентах. На каждой итерации метод BFGS обновляет аппроксимацию обратной матрицы Гессе на основе изменения градиентов и значений аргументов. После изменения аппроксимации данный метод использует полученное значение для определения направления. 
\subsubsection*{Реализация метода}
Реализация квазиньютоновского метода BFGS взята из библиотеки scipy.optimize, анализ эффективности которой будет рассмотрен далее.

\newpage
\section*{Сравнение эффективности методов}
\subsection*{Сравнение метода Ньютона с методом Newton-CG}
\subsubsection*{Выбор исследуемых функций}
1) $f = (1 - x)^{2} + 100 \cdot (y - x^{2})^{2}$ (функция Розенброка) \\

\noindent Данная функция имеет минимум в точке $(x, y) = (1, 1)$ (при $-1.5 \leq x \leq 4, -3 \leq y \leq 4$), где принимает значение равное $0$. Она является хорошим примером тестовой функции для сравнения эффективности работы каждого метода. \\

\noindent 2) $f = \sin(x + y) + (x - y)^2 - 1.5 \cdot x + 2.5 \cdot y + 1$ (функция МакКормика) \\

\noindent Данная функция имеет глобальный минимум в точке $(x, y) = (-0.54719, -1.54719)$, где принимает значение равное $-1.9133$. Она подойдет для проверки работоспособности каждого метода при различном выборе начальных точек.

\subsubsection*{Исследование точности и скорости}
\textbf{Начальные параметры:}
\begin{itemize}
    \item $f = (1 - x)^{2} + 100 \cdot (y - x^{2})^{2}$;
    \item $x_0 = (-2, 2)$;
    \item $\mathcal{E} \in \{10^{-5}, 10^{-10}\}$;
    \item $l = \mathcal{E}, r = 50$ (при использовании метода дихотомии).
\end{itemize}

\noindent \textbf{Точность полученных значений:} \\

\noindent \includegraphics[scale=0.30]{first_example_result2.jpeg} \\

\noindent \textbf{Количество итераций и обращений к функции при заданной точности:} \\

\noindent \includegraphics[scale=0.30]{first_example_result1.jpeg}

\begin{center}
    \includegraphics[scale=0.35]{first_example_standard_3d.png}
    \includegraphics[scale=0.30]{first_example_standard_2d.png}
    \captionof{figure}{Метод Ньютона} 
    \label{fig:enter-label}
\end{center}
\begin{center}
    \includegraphics[scale=0.35]{first_example_dichotomy_3d.png}
    \includegraphics[scale=0.30]{first_example_dichotomy_2d.png}
    \captionof{figure}{Метод Ньютона с использованием дихотомии} 
    \label{fig:enter-label}
\end{center}
\begin{center}
    \includegraphics[scale=0.35]{first_example_newton_cg_3d.png}
    \includegraphics[scale=0.30]{first_example_newton_cg_2d.png}
    \captionof{figure}{Метод Ньютона с использованием метода сопряженных градиентов} 
    \label{fig:enter-label}
\end{center}

\newpage
\subsubsection*{Исследование работоспособности в зависимости от выбранной начальной точки}
\textbf{Начальные параметры:}
\begin{itemize}
    \item $f = \sin(x + y) + (x - y)^2 - 1.5 \cdot x + 2.5 \cdot y + 1$;
    \item $x_0 \in \{(0, -3), (2.7, -1.5)\}$;
    \item $\mathcal{E} = 10^{-7}$;
    \item $l = \mathcal{E}, r = 100$ (при использовании метода дихотомии).
\end{itemize}
\textbf{Метод Ньютона}
\begin{center}
    \includegraphics[scale=0.25]{second_example_standard_3d.png}
    \includegraphics[scale=0.20]{second_example_standard_2d.png}
    \captionof{figure}{Некорректное значение при $x_0 = (0, -3)$} 
    \label{fig:enter-label}
\end{center}
\begin{center}
    \includegraphics[scale=0.25]{second_example_standard2_3d.png}
    \includegraphics[scale=0.20]{second_example_standard2_2d.png}
    \captionof{figure}{Некорректное значение при $x_0 = (2.7, -1.5)$} 
    \label{fig:enter-label}
\end{center}
\textbf{Метод Ньютона с использованием дихотомии}
\begin{center}
    \includegraphics[scale=0.25]{second_example_dichotomy_3d.png}
    \includegraphics[scale=0.20]{second_example_dichotomy_2d.png}
    \captionof{figure}{Корректное значение при $x_0 = (0, -3)$} 
    \label{fig:enter-label}
\end{center}
\begin{center}
    \includegraphics[scale=0.25]{second_example_dichotomy2_3d.png}
    \includegraphics[scale=0.20]{second_example_dichotomy2_2d.png}
    \captionof{figure}{Корректное значение при $x_0 = (2.7, -1.5)$} 
    \label{fig:enter-label}
\end{center}
\textbf{Метод Ньютона с использованием метода сопряженных градиентов}
\begin{center}
    \includegraphics[scale=0.25]{second_example_newton_cg_3d.png}
    \includegraphics[scale=0.20]{second_example_newton_cg_2d.png}
    \captionof{figure}{Корректное значение при $x_0 = (0, -3)$} 
    \label{fig:enter-label}
\end{center}
\begin{center}
    \includegraphics[scale=0.25]{second_example_newton_cg2_3d.png}
    \includegraphics[scale=0.20]{second_example_newton_cg2_2d.png}
    \captionof{figure}{Корректное значение при $x_0 = (2.7, -1.5)$} 
    \label{fig:enter-label}
\end{center}
\subsubsection*{Полученные результаты}
Из проведенных сравнений можно сделать вывод, что метод Ньютона довольно быстро находит минимум функции, однако его работоспособность очень зависит от выбора начальной точки, что и показало второе сравнение. Использование дихотомии, как и было сказано ранее, решает данную проблему, однако, в некоторых случаях, может требовать большего количества итераций алгоритма. Также стоит упомянуть, что данный метод все еще использует гессиан, вычисление и хранение которого очень затратно. Таким образом, исходя из полученных результатов, можно сказать, что использование сопряженных градиентов является выходом из этой ситуации, так как он сравним с использованием дихотомии по работоспособности и использует меньшее количество ресурсов.

\newpage
\subsection*{Сравнение эффективности методов: метода Нелдера-Мида, метод градиентного спуска, метод Ньютона и квазиньютоновских методов}

При рассмотрении метода Ньютона и метода градиентного спуска будем использовать дихотомию, так как в первой лабороторной работе мы нашли точку, начиная с которой, метод сходится.

\subsubsection*{Выбор исследуемой функции}
\noindent $f = -\cos(x) \cdot \cos(y) \cdot \exp(-((x - \pi)^2 + (y - \pi)^2))$ (функция Изома). \\

\noindent Данная функция имеет минимум в точке ${\displaystyle f(\pi ,\pi )=-1}$. \\

\noindent \textbf{Начальные параметры:}
\begin{itemize}
    \item $f = -\cos(x) \cdot \cos(y) \cdot \exp(-((x - \pi)^2 + (y - \pi)^2))$;
    \item $(x_0, y_0) = (1.71, 1.71)$;
    \item $\mathcal{E} = 10^{-5}$;
    \item $l = \mathcal{E}, r = 30$.
\end{itemize}

\noindent \textbf{Количество итераций и обращений к функции:} \\

\noindent \includegraphics[scale=0.50]{tabl-1.png} \\

\noindent \textbf{Метод градиентного спуска}
\begin{center}
    \includegraphics[scale=0.3]{grad-1.png}
    \includegraphics[scale=0.3]{grad-2.png}
    \captionof{figure}{} 
\end{center}

\newpage
\noindent \textbf{Метод Ньютона}
\begin{center}
    \includegraphics[scale=0.3]{newton-1.png}
    \includegraphics[scale=0.3]{newton-2.png}
    \captionof{figure}{} 
\end{center}

\noindent \textbf{Метод Нелдера-Мида}
\begin{center}
    \includegraphics[scale=0.3]{nelder-1.png}
    \includegraphics[scale=0.3]{nelder-2.png}
    \captionof{figure}{} 
\end{center}
\noindent \textbf{BFGS}
\begin{center}
    \includegraphics[scale=0.3]{bfgs-1.png}
    \includegraphics[scale=0.3]{bfgs-2.png}
    \captionof{figure}{} 
\end{center}

\noindent \textbf{Полученные результаты} \\

\noindent На данном примере видно, что метод Ньютона не смог найти минимум, так как начальное приближение оказалось не достаточно близким. Прим этом градиентный спуск был относительно легким в вычислении, метод Нелдера-Мида не использовал градиент, а BFGS потребовал меньше всего итераций. Плюсом этих методов стало и то, что они не использовали гессиан.

\subsection*{Сравнение эффективности методов нулевого порядка с квазиньютоновскими методами с вычислением производных разностным методом}
При большом количестве данных возникает проблема с получением, хранением и вычислением градиентов функции. В реализации квазиньютоновского метода BFGS из библиотеки scipy.optimize сделана возможность вычисления производных с помощью разностных методов, например:
\begin{itemize}
    \item $f'(x) \approx \dfrac{f(x + h) - f(x - h)}{2h}$ (2-point);
    \item $f'(x) \approx \dfrac{-f(x + 2h) + 4f(x + h) - 3f(x)}{2h}$ (3-point).
\end{itemize}
Таким образом, мы смогли избавится от явного получения и вычисления градиента функции, а значит, данный подход сравним с методом Нелдера-Мида из лабораторной работы №1. \\

\subsubsection*{Выбор исследуемой функции}
Хорошей функцией для сравнения данных методов будет функция типа Розенброка. В общем виде она определена так: $f = (a - x)^2 + b(y - x^2)^2$ и имеет глобальный минимум в точке $f(a, a^2) = 0$. Возьмем $a = 3, b = 20$, тогда минимум находится в точке $(3, 9)$. Сравним количество итераций каждого метода при начальной точке, которая довольно удалена от минимума. \\

\noindent \textbf{Начальные параметры:}
\begin{itemize}
    \item $f = (3 - x)^2 + 20 \cdot (y - x^2)^2$;
    \item $(x_0, y_0) = (-2.5, -7.5)$;
    \item $\mathcal{E} = 10^{-7}$.
\end{itemize}

\noindent \textbf{Метод Нелдера-Мида}
\begin{center}
    \includegraphics[scale=0.35]{main-3_nelder_mead_3d.png}
    \includegraphics[scale=0.3]{main-3_nelder_mead_2d.png}
    \captionof{figure}{} 
    \label{fig:enter-label}
\end{center}

\newpage
\noindent \textbf{Метод BFGS с вычислением производных разностным методом}
\begin{center}
    \includegraphics[scale=0.35]{main-3_bfgs_3d.png}
    \includegraphics[scale=0.3]{main-3_bfgs_2d.png}
    \captionof{figure}{2-point}
    \label{fig:enter-label}
\end{center}

\begin{center}
    \includegraphics[scale=0.35]{main-3_bfgs2_3d.png}
    \includegraphics[scale=0.3]{main-3_bfgs2_2d.png}
    \captionof{figure}{3-point}
    \label{fig:enter-label}
\end{center}

\noindent \textbf{Количество итераций и обращений к функции:} \\

\noindent \includegraphics[scale=0.30]{main-3_result.jpeg} \\

\noindent \textbf{Полученные результаты} \\

\noindent Из представленного сравнения можно сделать вывод, что оба метода требуют больше итераций, чем метод Ньютона, использующий гессиан. Однако можно выделить огромное преимущество, заключающееся в том, что метод Нелдера-Мида и метод BFGS (с вычислением производных разностным методом) работают без вычесленных ранее градиентов. Тем самым, мы полностью можем избавиться от проблемы большой затраты памяти.

\section*{Дополнительное задание 1}

\subsubsection*{Описание метода}

Одномерный поиск по правилу Вольфе - это метод численной оптимизации, используемый для нахождения шага при минимизации или максимизации функции. Он основан на сочетании условий убывания функции (например, условия Армихо) и условий кривизны (по формулировке Вольфа). Этот метод обеспечивает эффективное движение к оптимальной точке, учитывая как направление убывания, так и кривизну функции.
Условия Вольфе используются для выбора оптимального шага, удовлетворяющего двум условиям:
\begin{itemize}
    \item Условие Армихо: новое значение функции должно быть достаточно малым по сравнению со значением функции в текущей точке и должно быть больше, чем значение функции на предыдущем шаге.
     \item Условие кривизны гарантирует,что градиент в новой точке не слишком маленький по сравнению с градиентом в старой точке. Это помогает избежать слишком маленьких шагов и ускоряет сходимость градиентного спуска к оптимальному решению.
\end{itemize}

$f(x_k + \alpha \cdot p_k) \leq f(x_k) + c_1 \cdot \alpha \cdot grad (f^T_k) \cdot p_k$, где $ 0 < c_1 < 1$\\

$grad( f(x_k +  \alpha \cdot p_k)^T)\cdot p_k \geq c_2  grad (f^T_k) \cdot p_k$, где $ c_1 < c_2 < 1$

\subsubsection*{Метод Ньютона на основе метода Вольфе}

Метод Ньютона использует гессиан (матрицу вторых производных функции) для определения направления поиска, что позволяет более точно двигаться к точке минимума. Однако, прямое использование метода Ньютона может быть неэффективным из-за вычисления и обращения гессиана на каждом шаге, а также из-за чувствительности к выбору начальной точки и размеру шага.

\subsubsection*{Реализация метода}
На каждой итерации метода Ньютона мы используем одномерный поиск по правилу Вольфе для определения оптимального шага $\alpha$. Затем обновляем текущую точку по формуле:
\[x_{i+1} = x_i - \alpha \cdot H^{-1}(x_i) \cdot \nabla f(x_i)\]

\noindent \textbf{Начальные параметры:}
\begin{itemize}
    \item $f = (1 - x)^{2} + 100 \cdot (y - x^{2})^{2} $;
   \item $(x_0, y_0) = (-1, -1)$;
    \item $\mathcal{E} = 10^{-5}$;
\end{itemize}
\begin{center}
    \includegraphics[scale=0.35]{first_example_wolfe_3d.png}
    \includegraphics[scale=0.30]{first_example_wolfe_2d.png}
    \captionof{figure}{Метод Ньютона с использованием метода Вольфе на функции Розенброка} 
    \label{fig:enter-label}
\end{center}

\subsubsection*{Полученные результаты}

Метод Ньютона на основе метода Вольфе адаптирует размер шага $\alpha$ на каждой итерации с использованием одномерного поиска, что позволяет более точно и эффективно находить минимум функции, учитывая условия достаточного уменьшения и кривизны. Это улучшает сходимость метода и делает его более устойчивым к выбору начальной точки и размеру шага, в отличие от традиционного метода Ньютона, который может быть неэффективным из-за необходимости вычисления и обращения гессиана на каждом шаге.

\noindent Метод Ньютона с использованием сопряженных градиентов (Newton-CG) минимизирует необходимость прямого вычисления и обращения гессиана, используя приближенное решение системы линейных уравнений для определения направления поиска. Это делает Newton-CG особенно подходящим для задач большой размерности, где вычисление и хранение гессиана становится вычислительно затратным.

\noindent В сравнении, метод Ньютона на основе метода Вольфе предлагает баланс между точностью и вычислительной эффективностью, адаптируя размер шага для улучшения сходимости без необходимости обращения к методам приближенного решения, как в Newton-CG.


\section*{Дополнительное задание 2}

1) В основной части исследования (пункт 2) мы рассмотрели функцию Изома, которая является примеров и для этого пункта. Все графики и выводы можно увидеть \hyperlink{page.8}{тут}. \\

\noindent 2) Рассмотрим функция Химмельблау $f = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$. Она имеет 4 локальных минимума: 
\begin{itemize}
   \item $f(3,2)=0$;
   \item $f(-2{,}805118...,3{,}131312...)=0$;
   \item $f(-3{,}779310...,-3{,}283186...)=0$;
   \item $f(3{,}584428...,-1{,}848126...)=0$.
\end{itemize}

\noindent \textbf{Начальные параметры:}
\begin{itemize}
    \item $f = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$;
     \item $(x_0, y_0) = (-4, 0)$;
    \item $\mathcal{E} = 10^{-5}$;
    \item $l = \mathcal{E}, r = 30$.
\end{itemize}

\noindent \textbf{Метод градиентного спуска}
\begin{center}
    \includegraphics[scale=0.3]{ggrad-1.png}
    \includegraphics[scale=0.3]{ggrad-2.png}
    \captionof{figure}{$f(3.58437, -1.84809) = 0$} 
    \label{fig:enter-label}
\end{center}

\noindent \textbf{Метод Ньютона}
\begin{center}
    \includegraphics[scale=0.3]{nnewton-1.png}
    \includegraphics[scale=0.3]{nnewton-2.png}
    \captionof{figure}{$f(3, 2) = 0$} 
    \label{fig:enter-label}
\end{center}

\noindent \textbf{BFGS}
\begin{center}
    \includegraphics[scale=0.3]{bbfgs-1.png}
    \includegraphics[scale=0.3]{bbfgs-2.png}
    \captionof{figure}{$f(-3{,}779310...,-3{,}283186...)=0$} 
    \label{fig:enter-label}
\end{center}

\noindent \textbf{Метод Нелдера-Мида}
\begin{center}
    \includegraphics[scale=0.3]{mead-1.png}
    \includegraphics[scale=0.3]{mead-2.png}
    \captionof{figure}{$f(-2{,}805118...,3{,}131312...)=0$} 
    \label{fig:enter-label}
\end{center}

\noindent \textbf{Полученные результаты} \\

\noindent Метод Ньютона использует точное значение гессиана функции или его аппроксимацию для определения направления спуска. Он может быть более быстрым в сходимости к минимуму, чем градиентный спуск, но он также более чувствителен к выбору начальной точки и может застревать в локальных минимумах или точках седловых.\\ 

\noindent Градиентный спуск, с другой стороны, использует только информацию о градиенте функции, который указывает направление наискорейшего убывания. Это более простой метод, который может быть менее чувствителен к выбору начальной точки, но он может потребовать большего количества итераций для достижения минимума, особенно если функция имеет узкие долины или сильно вытянутые формы.Он требует меньше памяти, чем метод Ньютона.\\

\noindent Метод BFGS обновляет оценку обратного гессиана по мере продвижения по итерациям, используя информацию о градиентах. Он приближает гессиан итеративно и адаптивно, что делает его более эффективным для оптимизации в случаях, когда точное вычисление гессиана затруднительно или дорого. Он также обычно требует меньше памяти, чем метод Ньютона.\\

\noindent Метод Нелдера-Мида не использует информацию о градиенте или гессиане функции и оперирует только значениями функции в различных точках. Это позволяет ему исследовать пространство параметров более равномерно и гибко, что может помочь в обходе локальных минимумов, но при этом он может потребовать больше итераций для сходимости.\\

\end{document}
